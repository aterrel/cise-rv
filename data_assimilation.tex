\section{Example 2: Data Assimilation}

In the last section, we used random variables in simple examples involving
discrete random variables, in particular six sided dice. These problems were
solved internally by iteration as done by Erwig et al \cite{Erwig2006}. In this
section, we consider continuous random variables. Instead of iterating through
the continuum of possible values, we generate and solve integral expressions,
if possible. While the backend mechanics have been completely changed, the
method of setting up a statistical problem remains unchanged. This is an
advantage of integrating uncertainty in the language, a theme upon which we
will elaborate in the following sections.

Consider the problem of data assimilation. On a hot summer day, one might guess
the temperature outside to be about 30C. The uncertainity of this guess can be
encapsulated by an error of $\pm3$. In SymPy one can model model this estimate
with a normal random variable:

\begin{lstlisting}
>>> T = Normal(30, 3)
\end{lstlisting}

As in the last section, we may ask statistical questions such as ``What is the
probability that the temperature is greater than 33C?'' ($P(T>33)$). Such
questions produce integral expressions which are then solved using the SymPy
integration engine to produce numeric results.
\begin{eqnarray*}
P(T>33) & = & \int_{33}^{\infty} \frac{\sqrt{2} e^{- \frac{1}{18} \left(x_{3} -30\right)^{2}}}{6 \sqrt{\pi}}\, dx_{3} \\
& = & - \frac{1}{2} \operatorname{erf}{\left (\frac{1}{2} \sqrt{2} \right )} + \frac{1}{2} \\
& = & 0.15866
\end{eqnarray*}

Unsatisfied with this single estimate of the temperature, we want to update the
model with measurements from a thermometer. However, the thermometer is
difficult to read, because the lines are spaced closely together.  This means
our observations will include some noise, which we include in our model below:

\begin{lstlisting}
>>> noise = Normal(0, 1.5)
>>> observation = T + noise
\end{lstlisting}

The thermometer measures a value of $26C$. We now have two pieces of
information to model the temperature: the measured data and our \textit{prior}
understanding. We plot them both in Fig. \ref{fig:DA_data}.

\begin{figure}[ht]
\vspace{-0pt}
\centering
\includegraphics[width=.7\textwidth]{images/data.png}
\vspace{-0pt}
\caption{The probability density functions of our two pieces of data. Note that the observation is less uncertain and thus more centered about the mean.}
\label{fig:DA_data}
\vspace{00pt}
\end{figure}

Given the the original estimate, $30C \pm 3C$, and the new measurement $26C \pm
1.5C$, how should the final estimate of the temperature change?  You could use
only the thermometer's reading, because it is more precise. This method loses
the original information. To make use of the prior understanding, it would be best
to cleanly assimilate the new bit of data $(26C \pm 1.5C)$ into our prior
understanding $(30C \pm 3C)$.

This is the problem of Data Assimilation. We want to assimilate a new
measurement (data) into our previous understanding (prior) to form a new and
better-informed understanding (posterior). That is we want to compute $T' = T$
given an observation of $26C$. We compute this in SymPy as follows:

\begin{lstlisting}
>>> T_posterior = Given(T, Eq(observation, 26))
\end{lstlisting}

The new distribution is plotted in Fig. \ref{fig:DA_posterior}. The exact
functional form is proportional to $e^{-\frac{2}{9} \left(-x + 26\right)^{2}}
e^{-\frac{1}{18} \left(x-30\right)^{2}}$

\begin{figure}[ht]
\vspace{-0pt}
\centering
\includegraphics[width=.7\textwidth]{images/posterior.png}
\vspace{-0pt}
\caption{The density of the posterior represents our understanding after the observation has been assimilated into our prior understanding. Note that it is more centered/certain than either of the two. }
\label{fig:DA_posterior}
\vspace{00pt}
\end{figure}

This is a classic result in data assimilation. It is important to note that
these rules are not directly written into SymPy. Rather this is \textit{the}
answer given the most basic and fundamental rules.
